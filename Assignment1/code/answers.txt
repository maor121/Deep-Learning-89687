Student Name:
ID:

Answers to questions:

1)
Q: Were you able to get better results with the multi-layer perceptron?

A: No.

Firstly, the mlp1 model was added another improvement, otherwise it won't converge.
The initialization of weights was done with random values, not zeros. I did the grad_check for mlp1, so I believe the gradients are correct.
This was done because when the weights in mlp1 were initialized as 0, they weights will remain zero forever!
This is because the weight gradient calculation in the last layer is hidden_output*error, and if the hidden layer output is zero,
the weights gradient will remain 0.

Secondly, it seems mlp1 does not add extra accuracy on the dev set.
The answer for this is pretty simple actually. If we train loglinear enough, we can achieve 100% accuracy on the train_set.
Therefore, the loglinear has enough descriptive power to classify all the training set correctly. (It is overfitting actually)
But by adding another layer, (and more descriptive power), we only increase the chances of overfitting. So the accuracy on the train_set
remains the same, but on the dev set it deteriorates.

I believe that if we added Dropout to mlp1, the result would improve. (Same for loglinear actually, but overfitting is worse in mlp1).

Results:

train_loglin (with zero initial weights):
49 0.272165551876 0.945158526135 0.85           #Run 1
49 0.271820111708 0.932305055698 0.87           #Run 2

train_loglin (with random initial weights):
49 0.272830683422 0.953727506427 0.856666666667 #Run 1
49 0.267284910316 0.944730077121 0.856666666667 #Run 2

train_mlp1 (with random initial weights):
49 0.181911575221 0.959725792631 0.84           #Run 1
49 0.188292561831 0.875321336761 0.8033         #Run 2
49 0.195629323495 0.925449871465 0.8            #Run 3

train_mlp1 (with zero initial weights) - does not converge.

2)

Q: Switch to unigrams, what is the result in loglinear model? mlp1?

A: I added more epoches, and lowered the learning rate. But unigrams performed much worse than bigrams.
mlp1 performed slightly better then loglinear, but the diff was so small it is within error range, and not of any significance.

I believe it performed worse because bigrams hold more information then unigrams. Converting to unigrams looses all information
of letters order. A loss of information in the input is difficult making up for.

That assumption is further supported in 3grams, in which I was able to get 0.9 accuracy on the dev set.
I could not get this result with bigrams at all.

Results:

train loglin (with zero initial weights):
199 0.973512681027 0.691516709512 0.61

train loglin (with random initial weights):
199 0.970887519862 0.686803770351 0.6

train mlp1 (with random initial weights):
199 0.843959306146 0.705655526992 0.64

train mlp1 - 3grams (with random initial weights):
79 0.0894954601435 0.997857754927 0.913333333333

3)
